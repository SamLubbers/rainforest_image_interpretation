{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.base import TransformerMixin\n",
    "from skimage.io import imread, imread_collection_wrapper, concatenate_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from config import DATASETS_PATH, TRAIN_PATH, VALIDATION_PATH\n",
    "from helpers.utils import extract_label_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_file_pattern = \"*.tif\"\n",
    "train_imgs_path = os.path.join(TRAIN_PATH, img_file_pattern)\n",
    "\"\"\"\n",
    "it is necessary to create custom imread collection function which reads images with the 'imread' function\n",
    "in order to obtain the raw values from the tif image. \n",
    "The default imread_collection function returns images that are uncorrectly scaled between 0 and 255\n",
    "\"\"\"\n",
    "imread_collection_custom = imread_collection_wrapper(imread)\n",
    "train_imgs = imread_collection_custom(train_imgs_path, conserve_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spectral features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move classes extracting features to separate python file\n",
    "class BaseFeatureExtractor(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.pixels_axis = (1, 2)\n",
    "    \n",
    "    def fit(self, imgs, y=None):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def transform(self, imgs, y=None):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SpectralFeatureExtractor(BaseFeatureExtractor):\n",
    "    \"\"\"\n",
    "    extracts mean and standard deviation of every color channel in the image (RGB)\n",
    "    and the brightness, where brightness is defined as the mean of all color channels\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    imgs : numpy.ndarray\n",
    "           set of images, each with 4 channels (B, G, R, NIR)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def fit(self, imgs, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, imgs, y=None):\n",
    "        imgs = imgs[:, :, :, :3] # extract color channels\n",
    "        rgb_means = np.mean(imgs, axis=self.pixels_axis)\n",
    "        brightness = np.mean(rgb_means, axis=1)\n",
    "        brightness = np.reshape(brightness, (-1, 1))\n",
    "        rgb_sds = np.std(imgs, axis=self.pixels_axis)\n",
    "\n",
    "        return np.concatenate((rgb_means, brightness, rgb_sds), axis=1)\n",
    "\n",
    "class NDVIFeatureExtractor(BaseFeatureExtractor):\n",
    "    \"\"\"\n",
    "    extracts normalized difference vegatation index from multispectral image\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    imgs : numpy.ndarray\n",
    "           set of images, each with 4 channels (B, G, R, NIR)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def fit(self, imgs, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, imgs, y=None):\n",
    "        red = imgs[:, :, :, 2]\n",
    "        nir = imgs[:, :, :, 3]\n",
    "\n",
    "        ndvi = np.divide(nir-red, nir+red)\n",
    "        \n",
    "        ndvi_means = np.mean(ndvi, axis=self.pixels_axis)\n",
    "        ndvi_sds = np.std(ndvi, axis=self.pixels_axis)\n",
    "        ndvi_means = np.reshape(ndvi_means, (-1, 1))\n",
    "        ndvi_sds = np.reshape(ndvi_sds, (-1, 1))\n",
    "        return np.concatenate((ndvi_means, ndvi_sds), axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = FeatureUnion(transformer_list=[\n",
    "    (\"spectral\", SpectralFeatureExtractor()),\n",
    "    (\"ndvi\", NDVIFeatureExtractor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(imgs_collection, feature_extractor, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Extracts from imgs_collection the set of features specified in feature_extractor.\n",
    "    Extracts the features in batches because it is unviable to load all images at once into memory\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    imgs_collection : skimage.ImageCollection\n",
    "                      collection of images from which we want to extract the features\n",
    "    feature_extractor: sklearn transformer\n",
    "                       transformers that extract features from images\n",
    "    batch_size: number of images to extract features from at each iteration\n",
    "                a deafult value of 1000 loads approx. 0.5 GB into memory for this dataset\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    features: numpy.ndarray\n",
    "              set of features extracted from the image, \n",
    "              with one row for each image and one column for each feature\n",
    "    \"\"\"\n",
    "    \n",
    "    n_images = len(imgs_collection)\n",
    "    # get number of total features\n",
    "    features_im0 = feature_extractor.fit_transform(concatenate_images(train_imgs[:1]))\n",
    "    n_features = np.shape(features_im0)[1]\n",
    "    # create array for features\n",
    "    features = np.zeros([n_images, n_features])\n",
    "    features[0, :] = features_im0\n",
    "    for i in range(1, n_images, batch_size):\n",
    "        imgs_batch = concatenate_images(train_imgs[i:i+batch_size])\n",
    "        # casting to float64 is required to extract features! \n",
    "        # else \n",
    "        imgs_batch = imgs_batch.astype('float64', casting='safe')\n",
    "        features_batch = feature_extractor.fit_transform(imgs_batch)\n",
    "        features[i:i+batch_size, :] = features_batch\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = extract_features(train_imgs, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv(os.path.join(DATASETS_PATH, 'train_labels.csv'))\n",
    "train_labels = extract_label_values(train_labels)\n",
    "validation_labels = pd.read_csv(os.path.join(DATASETS_PATH, 'validation_labels.csv'))\n",
    "validation_labels = extract_label_values(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
